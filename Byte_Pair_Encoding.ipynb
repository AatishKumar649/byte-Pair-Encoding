{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzE0UG7GOJ+a+fzoHhkan0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AatishKumar649/byte-Pair-Encoding/blob/main/Byte_Pair_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BYTE PAIR ENCODING (BPE)**"
      ],
      "metadata": {
        "id": "9G0vS_MtNP12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented a simple tokenization scheme in the previous sections for illustration purposes.\n",
        "\n",
        "This section covers a more sophisticated tokenization scheme based on a concept called byte pair encoding (BPE).\n",
        "\n",
        "The BPE tokenizer covered in this section was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.\n",
        "\n",
        "Since implementing BPE can be relatively complicated, we will use an existing Python open-source library called tiktoken (https://github.com/openai/tiktoken).\n",
        "\n",
        "This library implements the BPE algorithm very efficiently based on source code in Rust."
      ],
      "metadata": {
        "id": "SwiAQM6JNXzZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkPfSCpceFKE",
        "outputId": "7a7dccdf-5078-4ea0-a936-c83566f5fce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip3 install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaeCllq-NgBZ",
        "outputId": "52c7068c-2d2b-4710-fcd6-19c51fd6ada6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:"
      ],
      "metadata": {
        "id": "YPHpp9UHNmbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "16WiBZK7NjwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqzWjItQNp7r",
        "outputId": "ded9099d-9131-48dc-ff85-191ddb48dd2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "The code above prints the following token IDs:\n",
        "\n",
        "</div>\n",
        "\n",
        "<div class=\"alert alert-block alert-success\">\n",
        "We can then convert the token IDs back into text using the decode method, similar to our\n",
        "SimpleTokenizerV2 earlier:</div>\n"
      ],
      "metadata": {
        "id": "KSns8o3uNvrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Lc9aIxsNsvW",
        "outputId": "bb38a414-0d59-4577-8469-ab5fcac21ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make two noteworthy observations based on the token IDs and decoded text above.\n",
        "\n",
        "First, the <|endoftext|> token is assigned a relatively large token ID, namely,\n",
        "\n",
        "In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50,257, with <|endoftext|> being assigned the largest token ID.\n",
        "\n",
        "Second, the BPE tokenizer above encodes and decodes unknown words, such as \"someunknownPlace\" correctly.\n",
        "\n",
        "The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens?\n",
        "\n",
        "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters.\n",
        "\n",
        "The enables it to handle out-ofvocabulary words.\n",
        "\n",
        "So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters\n",
        "\n",
        "Let us take another simple example to illustrate how the BPE tokenizer deals with unknown tokens"
      ],
      "metadata": {
        "id": "-r0xnvZWN3X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPRfPt6eNztg",
        "outputId": "bf7310aa-334e-4e33-efb1-f3184765aa7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREATING INPUT-TARGET PAIRS**"
      ],
      "metadata": {
        "id": "Gcc4Cb54jdEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach.\n",
        "To get started, we will first tokenize the whole The Verdict short story we worked with earlier using the BPE tokenizer introduced in the previous section:"
      ],
      "metadata": {
        "id": "4BfYe172jhhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqQ2TY8MN8t2",
        "outputId": "ba10aff3-acbd-4786-fe7b-b994cb4eb4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the code above will return 5145, the total number of tokens in the training set, after applying the BPE tokenizer.\n",
        "\n",
        "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it results in a slightly more interesting text passage in the next steps:"
      ],
      "metadata": {
        "id": "HBuRBDmZjlDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "2GFSRjmYhklh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1:\n",
        "The context size determines how many tokens are included in the input"
      ],
      "metadata": {
        "id": "pJnWdN-pjo1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U2LdbS9h2Ws",
        "outputId": "861d8c77-7a65-454d-8912-8768acb7c8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:"
      ],
      "metadata": {
        "id": "0cxiA6Ykjsdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBA4fi9Ch4b8",
        "outputId": "17b456d4-e388-4bdf-f7e4-4b0951a4e71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token ID on the right side of the arrow represents the target token ID that the LLM is supposed to predict.\n",
        "For illustration purposes, let's repeat the previous code but convert the token IDs into text:"
      ],
      "metadata": {
        "id": "lZqkvKbWjwMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y21oF1Dgh676",
        "outputId": "277d49f5-4b9a-4f24-ba27-156cd5bae00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've now created the input-target pairs that we can turn into use for the LLM training in upcoming chapters.\n",
        "\n",
        "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays.\n",
        "\n",
        "In particular, we are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict,"
      ],
      "metadata": {
        "id": "YUsAfUvej0V0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTING A DATA LOADER"
      ],
      "metadata": {
        "id": "TGKdW7ISiBSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and DataLoader classes.\n",
        "Step 1: Tokenize the entire text\n",
        "\n",
        "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "\n",
        "Step 3: Return the total number of rows in the dataset\n",
        "\n",
        "Step 4: Return a single row from the dataset"
      ],
      "metadata": {
        "id": "q96PITe9j39F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "UPTA43roh-gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPTDatasetV1 class in listing 2.5 is based on the PyTorch Dataset class.\n",
        "\n",
        "It defines how individual rows are fetched from the dataset.\n",
        "\n",
        "Each row consists of a number of token IDs (based on a max_length) assigned to an input_chunk tensor.\n",
        "\n",
        "The target_chunk tensor contains the corresponding targets.\n",
        "\n",
        "I recommend reading on to see how the data returned from this dataset looks like when we combine the dataset with a PyTorch DataLoader -- this will bring additional intuition and clarity.\n",
        "\n",
        "The following code will use the GPTDatasetV1 to load the inputs in batches via a PyTorch DataLoader:\n",
        "Step 1: Initialize the tokenizer\n",
        "\n",
        "Step 2: Create dataset\n",
        "\n",
        "Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
        "\n",
        "Step 4: The number of CPU processes to use for preprocessing"
      ],
      "metadata": {
        "id": "AFOcVeG-j7xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "ai2Cxv_yiEpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4,\n",
        "\n",
        "This will develop an intuition of how the GPTDatasetV1 class and the create_dataloader_v1 function work together:"
      ],
      "metadata": {
        "id": "Uo5yn5zYj_Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "qJqiv9lmiIgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function"
      ],
      "metadata": {
        "id": "vcH0QDQMkB64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9oSSefeiK7H",
        "outputId": "a49d9d41-76f3-42a3-a774-0ef17c5b480f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first_batch variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs.\n",
        "\n",
        "Since the max_length is set to 4, each of the two tensors contains 4 token IDs.\n",
        "\n",
        "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least 256.\n",
        "\n",
        "To illustrate the meaning of stride=1, let's fetch another batch from this dataset:"
      ],
      "metadata": {
        "id": "qQ08cH_SkGF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHNTdRPsiNVP",
        "outputId": "75115d44-ad0c-4f94-ba6b-3e5f71c9219c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we compare the first with the second batch, we can see that the second batch's token IDs are shifted by one position compared to the first batch.\n",
        "\n",
        "For example, the second ID in the first batch's input is 367, which is the first ID of the second batch's input.\n",
        "\n",
        "The stride setting dictates the number of positions the inputs shift across batches, emulating a sliding window approach\n",
        "\n",
        "Batch sizes of 1, such as we have sampled from the data loader so far, are useful for illustration purposes.\n",
        "\n",
        "If you have previous experience with deep learning, you may know that small batch sizes require less memory during training but lead to more noisy model updates.\n",
        "\n",
        "Just like in regular deep learning, the batch size is a trade-off and hyperparameter to experiment with when training LLMs.\n",
        "\n",
        "Before we move on to the two final sections of this chapter that are focused on creating the embedding vectors from the token IDs, let's have a brief look at how we can use the data loader to sample with a batch size greater than 1:"
      ],
      "metadata": {
        "id": "E4ICjluekLQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94fTyBmIiQRr",
        "outputId": "4320a168-87a0-4f3e-cbcd-7d7955b790a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we increase the stride to 4. This is to utilize the data set fully (we don't skip a single word) but also avoid any overlap between the batches, since more overlap could lead to increased overfitting."
      ],
      "metadata": {
        "id": "jV6dJk50kO7G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPWsAvl0iSxj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}